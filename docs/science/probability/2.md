# 概率入门

以下部分内容来自 [**OI Wiki**](https://oi-wiki.org/)。

## 概率论基本概念

### 样本空间

简而言之，样本空间 $\Omega$ 指明随机现象**所有**可能出现的结果。

具体的，一个随机现象中可能发生的不能再细分的结果被称为**样本点**，所有样本点的集合称为**样本空间**，通常用 $\Omega$ 来表示。

二维样本空间的列举，表格法：

|       |    1      |    2      |    3      |    4      |    5      |    6      |
|:-----:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| **1** | $(1, 1)$ | $(1, 2)$ | $(1, 3)$ | $(1, 4)$ | $(1, 5)$ | $(1, 6)$ |
| **2** | $(2, 1)$ | $(2, 2)$ | $(2, 3)$ | $(2, 4)$ | $(2, 5)$ | $(2, 6)$ |
| **3** | $(3, 1)$ | $(3, 2)$ | $(3, 3)$ | $(3, 4)$ | $(3, 5)$ | $(3, 6)$ |
| **4** | $(4, 1)$ | $(4, 2)$ | $(4, 3)$ | $(4, 4)$ | $(4, 5)$ | $(4, 6)$ |
| **5** | $(5, 1)$ | $(5, 2)$ | $(5, 3)$ | $(5, 4)$ | $(5, 5)$ | $(5, 6)$ |
| **6** | $(6, 1)$ | $(6, 2)$ | $(6, 3)$ | $(6, 4)$ | $(6, 5)$ | $(6, 6)$ |

### 随机事件

一个**事件**是样本空间 $\Omega$ 的**任意子集**，又分为：

- 一个**随机事件**是样本空间 $\Omega$ 的**非空真子集**。
- 一个**必然事件**是样本空间 $\Omega$ 本身。
- 一个**不可能事件**是一个空集 $\varnothing$。
- 一个**基本事件**是样本空间 $\Omega$ 的一个大小为 $1$ 的子集。

由此可知，事件是一个由若干样本点构成，用大写字母 $A, B, C, \cdots$ 表示。

对于一个随机现象的结果 $\omega$ 和一个随机事件 $A$，我们称事件 $A$ 发生了 当且仅当 $\omega \in A$。

例如：掷一次骰子得到的点数是一个随机现象，其样本空间可以表示为 $\Omega=\{1,2,3,4,5,6\}$。设随机事件 $A$ 为「获得的点数大于 $4$」，则 $A = \{ 5, 6 \}$。若某次掷骰子得到的点数 $\omega = 3$，由于 $\omega \notin A$，故事件 $A$ 没有发生。

### 事件的运算

由于我们将随机事件定义为了样本空间 $\Omega$ 的子集，故我们可以将集合的运算（如交、并、补等）移植到随机事件上。记号与集合运算保持一致。

- 并（和）事件：事件的并 $A \cup B$ 也可记作 $A + B$，表示至少有一个事件发生。
- 交（积）事件：事件的交 $A \cap B$ 也可记作 $AB$，表示事件全部发生。

## 概率的定义和性质

### 古典概型

在概率论早期实践中，由于涉及到的随机现象都比较简单，具体表现为样本空间 $\Omega$ 是有限集，且直观上所有样本点是等可能出现的，因此人们便总结出了下述定义（称为**传统概率模型**或**古典概率模型**或**拉普拉斯概率模型**）：

如果一个随机现象满足：

- 只有有限个基本结果。
- 每个基本结果出现的**可能性是一样的**。

那么对于每个事件 $A$，定义它的概率为：

$$
P(A)=\dfrac{|A|}{|\Omega|}
$$

最经典的例子是，掷硬币、掷骰子。

或者用 $\#(\cdot)$ 表示对随机事件（一个集合）大小的度量：

$$
P(A)=\dfrac{\#(A)}{\#(\Omega)}
$$

后来人们发现这一定义可以直接推广到 $\Omega$ 无限的一部分情景中，于是就有了所谓几何概型。

在古典概型中，最应当注意的是**一致的可能性**，例如扔两次硬币，一正一反就不应当是一个于两正、两反等概率的事件。

### 几何概型

在这个模型下，随机实验所有可能的结果是无限的，并且每个基本结果发生的**概率是相同的**。

几何概型定义，概率 $=$ 有利区域测度 $\div$ 总区域测度。当所求解问题可以转化为某种随机分布的特征数，比如随机事件出现的概率，或者随机变量的期望，就可以使用蒙特卡罗法。

通过大量随机抽样的方法，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，并将其作为问题的解。

经常的，我们会因为概率相同犯错误，这也导致了 [Bertrand（伯特兰）悖论](https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)) 等问题的产生，于是也就诞生了概率的公理化描述。

### 概率公理

公理一：$0\le P(A)\le1(A\subset\Omega)$。

公理二：$P(\Omega)=1,P(\varnothing)=0$。

公理三：$A\cap B=\varnothing\iff P(A\cup B)=P(A)+P(B)$。

推论：

- 若 $A\subset B$，则 $P(A)<P(B)$（概率的单调性）。

- 若 $A$ 与 $B$ 对立，则 $P(A)+P(B)=1$。

- 容斥原理：$P(A\cup B)=P(A)+P(B)-P(A\cap B)$。

其中上面第二条就是容斥原理的推论。

### 频率学派

频率学派强调通过数据出现的**频率**或比例，从样本数据中得出结论。

根据大数定律，样本数量越多，则其算术平均值就有越高的概率接近期望。

最经典的例子是，抛硬币正面向上的频率趋近于 $0.5$。

### 主观概率

主观概率，是指建立在过去的经验与判断的基础上，根据对未来事态发展的预测和历史统计资料的研究确定的概率。主观概率反映的只是一种主观可能性，尽管有一定的科学性，但和能客观地反映事物发展规律的自然概率不同。

最经典的例子是，降雨概率。

## 条件概率

### 条件概率

当某事件已经发生时，一些随机事件的概率会因为已知信息的增加发生变化。

若已知事件 $A$ 发生，在此条件下事件 $B$ 发生的概率称为 条件概率，记作 $P(B|A)$。

在样本空间中，若事件 $A$ 满足 $P(A) > 0$，则条件概率 $P(\cdot|A)$ 定义为：

$$
P(B|A) = \frac{P(AB)}{P(A)}
$$

条件概率有时候也称为**后验概率**，与先验概率相对。

1. $P(\Omega|A)=1$.

2. 若 $B,C$ 互斥（$BC=\varnothing$）则：

    $$
    P(BC)=P(B)+P(C)
    $$

    $$
    P(PC|A)=P(B|A)+P(C|A)
    $$

    $$
    P(\bar B|A)=1-P(B|A)
    $$

条件概率的计算有还有三个公式，我们详细讲解。

### 概率乘法公式

若 $P(A) > 0$，则对任意事件 $B$ 都有

$$
P(AB) = P(A)P(B|A)
$$

注意到这也就是条件概率的定义式。

### 全概率公式

全概率公式指出，对于 $A,B$ 两组对立事件，

$$
P(B)=P(A)P(B|A)+P(\bar A)P(B|\bar A)
$$

可以理解为，$A$ 发生后 $B$ 发生，和 $A$ 不发生但是 $B$ 发生概率之和。

In general，若一组事件 $A_1, \cdots, A_n$ 共同对立（两两不交、互相独立且和为 $\Omega$），则对任意事件 $B$ 都有：

$$
P(B) = \sum_{i=1}^{n} P(A_i)P(B|A_i)
$$

### Bayes 公式

贝叶斯定理（也成贝氏定理）指出，若 $P(A),P(B)>0$，则：

$$
P(A|B)=\dfrac{P(AB)}{P(B)}=\dfrac{P(A)P(B|A)}{P(B)}
$$

可以理解为将中间的 $P(AB)$ 用概率乘法公式展开，向左向右写出。

也可以将 $P(A)$ 提出来，剩余的部分 $P(B|A)/P(B)$ 称为标准似然度。

带入全概率公式，于是有：

$$
P(A|B)=\dfrac{P(A)P(B|A)}{P(A)P(B|A)+P(\bar A)P(B|\bar A)}
$$

一般来说，设可能导致事件 $B$ 发生的原因为 $A_1, A_2, \cdots, A_n$（同样构成了**互斥**），则在 $P(A_i)$ 和 $P(B|A_i)$ 已知时可以通过全概率公式计算事件 $B$ 发生的概率。但在很多情况下，我们需要根据「事件 $B$ 发生」这一结果反推其各个原因事件的发生概率。

$$
P(A_i|B) = \frac{P(A_iB)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{j=1}^{n} P(A_j)P(B|A_j)}
$$

## 事件的独立性

### 互斥和对立事件

**互斥事件**：$P(AB)=0$，即有 $A$ 没 $B$ 有 $B$ 没 $A$。

$$
A,B\textsf{ 互斥}\iff AB=\varnothing
$$

**对立事件**：其中必有一个发生的两个互斥事件。

$$
A,B\textsf{ 对立}\iff AB=\varnothing,A\cup B=\Omega
$$

对于互斥事件和对立事件（是互斥事件的一个特例）：

$$
P(AB)=P(A)+P(B)
$$

### 独立事件和独立性

**独立事件**：$A$ 发生不影响 $B$ 而 $B$ 发生也不影响 $A$。

$$
P(AB)=P(A)P(B)
$$

根据这个式子，如果 $A,B$ 独立，那么 $A$ 及其补集，$B$ 及其补集也应当都是独立的。

在条件概率中，若 $A,B$ 独立：

$$
P(A|B)=\dfrac{P(AB)}{P(B)}=P(A)
$$

$$
P(B|A)=\dfrac{P(AB)}{P(A)}=P(B)
$$

也可以用条件概率推导独立，这是 iff 的。

### 多个事件的独立性

对于多个事件 $A_1, A_2, \cdots, A_n$，我们称其独立，当且仅当对任意一组事件 $\{ A_{i_k} : 1 \leq i_1 < i_2 < \cdots < i_k \leq n \}$ 都有：

$$
P( A_{i_1}A_{i_2} \cdots A_{i_r} ) = \prod_{k=1}^{r} P(A_{i_k})
$$

对于多个事件，一般不能从两两独立推出这些事件独立。考虑以下反例：

- 有一个正四面体骰子，其中三面被分别涂成红色、绿色、蓝色，另一面则三色皆有。现在扔一次该骰子，令事件 $A,B,C$ 分别表示与桌面接触的一面包含红色、绿色、蓝色。

不难计算：

$$
P(A) = P(B) = P(C) = \frac{1}{2}
$$

$$
P(AB) = P(BC) = P(CA) = P(ABC) = \frac{1}{4}
$$

显然 $A, B, C$ 两两独立，但由于 $P(ABC) \neq P(A)P(B)P(C)$，故 $A, B, C$ 不独立。

## 概率的应用

### 条件概率谬论

条件概率的谬论是假设 $P(A|B)$ 大致等于 $P(B|A)$。

根据 Bayes 公式：

$$
P(A)P(B|A)=P(B)P(A|B)
$$

最经典的例子是患病概率，考虑到灵敏度、特异度等因素，本文不予讲解，详见 [Wikipedia](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)。
